# GPT2 training data extractor


### Purpose 

Repository to experiment on language model memorization.

### Source and references
The code in this repository is a refactored version of the 
[Training data extraction from GPT-2](https://github.com/ftramer/LM_Memorization)
repository. 

### Setup 

It is recommended to setup a conda virtual environment with the pytorch and Transformers libraries.
The file [setup.txt](https://github.com/rcuevass/lang_model_mem_gpt2/blob/main/setup.txt) specifies how to accomplish this setup.
