# GPT2 training data extractor


### Purpose 

Repository to experiment on language model memorization.

### Source and references

* The code in this repository is an in-progress refactored version of the 
[Training data extraction from GPT-2](https://github.com/ftramer/LM_Memorization)
repository. 
* Papers related to the work tested with this code can be found in the [references](https://github.com/rcuevass/lang_model_mem_gpt2/tree/main/references) folder of this repo.
* The folder [output](https://github.com/rcuevass/lang_model_mem_gpt2/tree/dev/output) contains examples of training data extracted from GPT2 language model.

### Setup 

It is recommended to setup a conda virtual environment with the pytorch and Transformers libraries.
The file [setup.txt](https://github.com/rcuevass/lang_model_mem_gpt2/blob/main/setup.txt) specifies how to accomplish this setup.
